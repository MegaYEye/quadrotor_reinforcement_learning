\documentclass{article}

\usepackage[nottoc]{tocbibind}
\usepackage[a4paper, total={7.7in, 11in}]{geometry}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amssymb}
\usepackage{chngpage}
\DeclareMathOperator{\E}{\mathbb{E}}
\DeclareMathOperator{\D}{\mathbb{D}}

\begin{document}

\title{Machine Learning Project 2018: With Cherry Flavor}
\maketitle

\section{Introduction}

\cite{cad2rl}


Here will go  brief problem formulation.

\subsection{Reinforcement Learning}

Here we explain what is Reinforcement Learning.

\subsection{Unreal Engine}

\subsection{AirSim}

Recently, paradigms such as reinforcement learning, learning-by-demonstration
and transfer learning are proving a natural means to train various robotics
systems. One of the key challenges with these techniques is the high sample 
complexity - the amount of training data needed to learn useful behaviors is often 
prohibitively high. This issue is further exacerbated by the fact that autonomous 
vehicles are often unsafe and expensive to operate during the training phase. In order to
seamlessly operate in the real world, the robot needs to transfer the learning it does
in simulation. Currently, this is a non-trivial task as simulated perception, environments 
and actuators are often simplistic and lack the richness or diversity of the
real world. For example, for robots that aim to use computer vision in outdoor 
environments, it may be important to model real-world complex objects such as trees,
roads, lakes, electric poles and houses along with rendering that includes finer 
details such as soft shadows, specular reflections, diffused inter-reflections and so on.
Similarly, it is important to develop more accurate models of system dynamics so
that simulated behavior closely mimics the real-world.

AirSim  is  an  open-source  platform \cite{airsim2017fsr} by Microsoft, 
that  aims  to  narrow  the  gap  between
simulation  and  reality  in  order  to  aid  development  of  autonomous  vehicles.  
The platform is based on the Unreal Engine, and seeks to positively 
influence development and testing of data-driven
machine intelligence techniques such as reinforcement learning and 
deep learning. It is inspired by several previous simulators (see 
related work), and proved to be very useful in this project development.

\section{Description of the framework Architecture}

Architecture consists of the following classes. The main character is Agent, which can act, observe and train.
To make agent more readable, we separated History and ExperienceReplayMemory into separated classes.
With the goal of making substitutable actions and rewards, we introduced classes ActionSpace and Reward.
To control exploration-exploatation dilemma, we created module to handle exploration (with Explorer classes).

Our implementation uses deep learning libraries CNTK and TensorFlow (with Tensorboard).

\section{Considered Environments}

\section{Agent: Deep Q-learning}

\subsection{General description} We are based on the deep reinforcement
Q-learning, which was first described in \cite{mnih2013playing}.
After a while, more sophisticated version was published in 
\cite{mnih2015humanlevel}.

\subsection{Network architecture}

\subsection{Applied tweaks}

\begin{itemize}
        \item Experience Replay.
        \item Freezed network to reduce correlation.
        \item TD Update 
        \item Huber Loss
        \item 4-frame delayed learning
\end{itemize}


\section{Implemented Rewards}

\begin{itemize}
    \item {\bf Exploration Reward.}
    \item {\bf Landscape Reward.}
    \item {\bf Path Reward.}
    \item {\bf Corridor Reward.}
\end{itemize}

\section{Implemented Action Spaces}

\begin{itemize}
    \item {\bf Grid Action Space.}
    \item {\bf Flat Action Space.}
    \item {\bf Canonical (default) Action Space.}
    \item {\bf Corridor Action Space.}
\end{itemize}

\section{Exploration}

\begin{itemize}
    \item {\bf Constant $\varepsilon$-greedy.}
    \item {\bf Simulated annealing $\varepsilon$-greedy.}
\end{itemize}

\section{Results}


\section{Team Contribution}

\bibliographystyle{unsrt}
\bibliography{reference}

\end{document}

