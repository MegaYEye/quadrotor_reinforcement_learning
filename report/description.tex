\documentclass{article}

\usepackage[nottoc]{tocbibind}
\usepackage[a4paper, total={7.7in, 11in}]{geometry}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amssymb}
\usepackage{chngpage}
\DeclareMathOperator{\E}{\mathbb{E}}
\DeclareMathOperator{\D}{\mathbb{D}}

\begin{document}

\title{Machine Learning Project 2018: With Cherry Flavor}
\maketitle

\section{Introduction}

\cite{cad2rl}


Here will go  brief problem formulation.

\subsection{Reinforcement Learning}

Here we explain what is Reinforcement Learning.

\subsection{Unreal Engine}

\subsection{AirSim}

Here we explain what is AirSim.
\cite{airsim2017fsr}

\section{Description of the framework Architecture}

Architecture consists of the following classes. The main character is Agent, which can act, observe and train.
To make agent more readable, we separated History and ExperienceReplayMemory into separated classes.
With the goal of making substitutable actions and rewards, we introduced classes ActionSpace and Reward.
To control exploration-exploatation dilemma, we created module to handle exploration (with Explorer classes).

Our implementation uses deep learning libraries CNTK and TensorFlow (with Tensorboard).

\section{Considered Environments}

\section{Agent: Deep Q-learning}

\subsection{General description} We are based on the deep reinforcement
Q-learning, which was first described in \cite{mnih2013playing}.
After a while, more sophisticated version was published in 
\cite{mnih2015humanlevel}.

\subsection{Network architecture}

\subsection{Applied tweaks}

\begin{itemize}
        \item Experience Replay.
        \item Freezed network to reduce correlation.
        \item TD Update 
        \item Huber Loss
        \item 4-frame delayed learning
\end{itemize}


\section{Implemented Rewards}

\begin{itemize}
    \item {\bf Exploration Reward.}
    \item {\bf Landscape Reward.}
    \item {\bf Path Reward.}
    \item {\bf Corridor Reward.}
\end{itemize}

\section{Implemented Action Spaces}

\begin{itemize}
    \item {\bf Grid Action Space.}
    \item {\bf Flat Action Space.}
    \item {\bf Canonical (default) Action Space.}
    \item {\bf Corridor Action Space.}
\end{itemize}

\section{Exploration}

\begin{itemize}
    \item {\bf Constant $\varepsilon$-greedy.}
    \item {\bf Simulated annealing $\varepsilon$-greedy.}
\end{itemize}

\section{Results}


\section{Team Contribution}

\bibliographystyle{unsrt}
\bibliography{reference}

\end{document}

