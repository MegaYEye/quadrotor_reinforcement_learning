\documentclass{article}

\usepackage[nottoc]{tocbibind}
\usepackage[a4paper, total={7.7in, 11in}]{geometry}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amssymb}
\usepackage{chngpage}
\DeclareMathOperator{\E}{\mathbb{E}}
\DeclareMathOperator{\D}{\mathbb{D}}

\begin{document}

\title{Machine Learning Project 2018: With Cherry Flavor}
\maketitle

\small

\section{Introduction}

Indoor  navigation  and  collision  avoidance  is  one  of  the
basic  requirements  for  robotic  systems  that  must  operate  in
unstructured  open-world  environments,  including  quadrotors,
mobile  manipulators,  and  other  mobile  robots.  Many  of  the
most successful approaches to indoor navigation have used
mapping  and  localization  techniques  based  on  3D  
perception,  including  SLAM,  depth  sensors,  stereo  cameras,  
and  monocular  cameras  using  structure  from  motion. 
The use of sophisticated sensors and specially mounting multiple 
cameras on the robot imposes additional costs on
a  robotic  platform,  which  is  a  particularly  prominent  issue
for weight and power constrained systems such as lightweight
aerial vehicles. Monocular cameras, on the other hand, require
3D  estimation  from  motion,  which  remains  a  challenging
open  problem  despite  considerable  recent  progress.
In this project we are based on the paper \cite{cad2rl}, 
and explore a learning-based approach for indoor
navigation,  which  directly  predicts  collision-free
commands from monocular images, without attempting to 
explicitly  model  or  represent  the  3D  structure  
of  the  environment.

To model the environment, we use Unreal Engine and AirSim.

\subsection{Unreal Engine}

The Unreal Engine is a game engine developed by Epic Games, 
first showcased in the 1998 first-person shooter game Unreal. 
Although primarily developed for first-person shooters, it has 
been successfully used in a variety of other genres, including 
stealth, fighting games, MMORPGs, and other RPGs. With its code 
written in C++, the Unreal Engine features a high degree of portability 
and is a tool used by many game developers today. It has won several 
awards, including the Guinness World Records award 
for "most successful video game engine."

\subsection{AirSim}

Recently, paradigms such as reinforcement learning, learning-by-demonstration
and transfer learning are proving a natural means to train various robotics
systems. One of the key challenges with these techniques is the high sample 
complexity - the amount of training data needed to learn useful behaviors is often 
prohibitively high. This issue is further exacerbated by the fact that autonomous 
vehicles are often unsafe and expensive to operate during the training phase. In order to
seamlessly operate in the real world, the robot needs to transfer the learning it does
in simulation. Currently, this is a non-trivial task as simulated perception, environments 
and actuators are often simplistic and lack the richness or diversity of the
real world. For example, for robots that aim to use computer vision in outdoor 
environments, it may be important to model real-world complex objects such as trees,
roads, lakes, electric poles and houses along with rendering that includes finer 
details such as soft shadows, specular reflections, diffused inter-reflections and so on.
Similarly, it is important to develop more accurate models of system dynamics so
that simulated behavior closely mimics the real-world.

AirSim  is  an  open-source  platform \cite{airsim2017fsr} by Microsoft, 
that  aims  to  narrow  the  gap  between
simulation  and  reality  in  order  to  aid  development  of  autonomous  vehicles.  
The platform is based on the Unreal Engine, and seeks to positively 
influence development and testing of data-driven
machine intelligence techniques such as reinforcement learning and 
deep learning. It is inspired by several previous simulators (see 
related work), and proved to be very useful in this project development.

\section{Project description and approach using Reinforcement Learning}

\section{Description of the framework Architecture}

Architecture consists of the following classes. The main character is Agent, which can act, observe and train.
To make agent more readable, we separated History and ExperienceReplayMemory into separated classes.
With the goal of making substitutable actions and rewards, we introduced classes ActionSpace and Reward.
To control exploration-exploatation dilemma, we created module to handle exploration (with Explorer classes).

Our implementation uses deep learning libraries CNTK and TensorFlow (with Tensorboard).

\section{Considered Environments}

\section{Agent: Deep Q-learning}

\subsection{General description} We are based on the deep reinforcement
Q-learning, which was first described in \cite{mnih2013playing}.
After a while, more sophisticated version was published in 
\cite{mnih2015humanlevel}.

\subsection{Network architecture}

\subsection{Applied tweaks}

\begin{itemize}
    \item Experience Replay.
    \item Freezed network to reduce correlation.
    \item TD Update 
    \item Huber Loss
    \item 4-frame delayed learning
\end{itemize}

\section{Implemented Rewards}

\begin{itemize}
    \item {\bf Exploration Reward.}
    \item {\bf Landscape Reward.}
    \item {\bf Path Reward.}
    \item {\bf Corridor Reward.}
\end{itemize}

\section{Implemented Action Spaces}

\begin{itemize}
    \item {\bf Grid Action Space.}
    \item {\bf Flat Action Space.}
    \item {\bf Canonical (default) Action Space.}
    \item {\bf Corridor Action Space.}
\end{itemize}

\section{Exploration}

\begin{itemize}
    \item {\bf Constant $\varepsilon$-greedy.}
    \item {\bf Simulated annealing $\varepsilon$-greedy.}
\end{itemize}

\section{Results}


\section{Team Contribution}

\bibliographystyle{unsrt}
\bibliography{reference}

\end{document}

